# Методы математической статистики

## 2. Метод моментов

Метод моментов используется для широкого класса моделей, при условии существования всех теоретических моментов с 1-го по m-й, где m - число оцениваемых параметров.

- $\mu_{k}$ - начальный момент k-го порядка.
- $m_{k}$ - выборочный начальный момент.

Если $\mu_{k}$ существует, то по закону больших чисел (ЗБУ):

$$m_{k} \to \mu_{k}, \text{ } n \to \infty$$

Пусть распределение выборки зависит от m параметров:

$$\theta = (\theta_{1}, \dots, \theta_{m}) \in \Theta$$

где $\Theta$ - некоторая область в $R^{m}$. 

Если существуют:

$$\mu_{k} = \mu(\theta), k = \overline{1,m}$$

можно записать систему нелинейных уравнений:

$$\mu_{k}(\theta) = y_{k}$$

Пусть левая часть системы задает взаимно-однозначное отображение:

$$g: \Theta \to B$$

где $B$ - некоторая область в $R^{m}$, и обратное отображение:

$$g^{-1}: B \to \Theta \text{ непрерывно}$$

Для всех $y$ из $B$ система имеет единственное решение, которое непрерывно зависит от правой части. 

Компоненты решения системы:

$$\hat{\theta} = (\hat{\theta_{1}}, \dots, \hat{\theta_{m}})$$

при $y_{k} = m_{k}$ называются оценками метода моментов (MM), где $\theta_{m}$ - случайные величины, так как $m_{k}$ - функция от случайной выборки.

### Примеры

**Пример 1.** Непрерывное равномерное распределение на $[0, \theta]$:

$$M\epsilon = \frac{\theta}{2} = \overline{x} \to \hat{\theta} = 2\overline{x}$$

**Пример 2.** Гамма распределение $\theta_{1}, \theta_{2}$:

$$\mu_{1} = \frac{\theta_{2}}{\theta_{1}} = \overline{x}$$

$$\theta_{2} = \frac{\theta_{2}(\theta_{2}+1)}{\theta^2} = \overline{x^2}$$

$$\hat{\theta_{2}} = \frac{(\overline{x})^2}{S^2}$$

$$\hat{\theta_{1}} = \frac{\overline{x}}{S^2}$$

## 3. Метод максимального правдоподобия (ММП)

Для простоты предположим дискретное распределение:

$$P(\epsilon_{i} = x) = f(x, \theta)$$

Совместная вероятность выборки:

$$f(x_{1}, \dots, x_{n}, \theta) = f(x_{1}, \theta) * \dots * f(x_{n}, \theta)$$

зависит от $n + m$ аргументов. Если её рассматривать как функцию от $\theta$ при фиксированной выборке, то она называется **функцией правдоподобия**:

$$L(\theta) = \prod f(x_{1}, \theta)$$

Разумно в качестве оценок взять такие значения, которые обеспечивают максимум правдоподобия:

$$argmax L(\theta) \text{ - оценка ММП}$$

Удобно работать с логарифмом функции правдоподобия: $\ln L(\theta)$.

### Примеры

**Пример 1:** Схема Бернулли с вероятностью успеха $\theta$:

$$\ln L(\theta) = \ln \theta \sum_{i=1}^{n}x_{i} + \left( n - \sum_{i=1}^{n}x_{i} \right)\ln(1 - \theta)$$

$$\frac{d\ln L(\theta)}{d\theta} = 0 \text{ - так можно делать, когда L - гладкая}$$

$$\hat{\theta} = \frac{\sum_{i=1}^{n}x_{i}}{n} = \overline{x}$$

**Пример 2:** Дискретное равномерное распределение на $[0, \theta]$:

$$P(\epsilon_{i} = x) = \frac{1}{\theta + 1}|_{0 \leq x \leq \theta}$$

$$L(\theta) = \frac{1}{(\theta + 1)^n}|_{X_{n} \leq \theta}$$

$$maxL(\theta) \text{ в точке } X_{(n)}$$

Оценка ММП в примере 2 смещена. Несмещенная оценка:

$$\hat{\theta} = \frac{n + 1}{n}x_{(n)}$$

Если $n \to \infty$, то оценка ММП - асимптотически несмещенная. Иногда решение можно найти явно, но чаще численно.

### Свойства ММП-оценок

1. Имеют наименьшую асимптотическую дисперсию (неравенство Рао-Крамера) $\to$ асимптотическая эффективность при выполнении условий регулярности.
2. При выполнении условий регулярности при больших и сильно состоятельных $\hat{\theta}_{n} \to \theta, n \to \theta$.

## Доверительные интервалы

Вместо точечной оценки (число) для $\theta$ можно указать интервал $(\hat{\theta}_{min}, \hat{\theta}_{max})$, в которых $\theta$ попадает с заданной (**доверительной**) вероятностью - доверительный интервал (или интервальная оценка).

Малая величина $\alpha = 1$ - доверительная вероятность, уровень значимости. Обычно $\alpha = 0.05; 0.01; 0.005$.

Статистики $\hat{\theta}_{min}, \hat{\theta}_{max}$ - границы интервала с доверительной вероятностью $(1 - \alpha)$, если $\forall \theta \in \Theta$ для выборки $\epsilon_1, \dots, \epsilon_{n}$ из $F_{\theta}(x)$ справедливо:

$$P(\hat{\theta}_{min}(\epsilon_{1}, \dots, \epsilon_{n}) < \theta < \hat{\theta}_{max}(\epsilon_{1}, \dots, \epsilon_{n})) \geq 1 - \alpha$$

При повышении доверительной вероятности длина интервала повышается. Доверительный интервал зависит от распределения $\hat{\theta}$, которое зависит от распределения выборки. Самый простой случай - нормальная модель.

### Интервалы в нормальной модели

$$\epsilon_{i} \sim N(\theta, \sigma^2)$$

где $\sigma^2$ известно. Оценим параметр сдвига. Такая модель часто применяется к данным, полученным при независимых измерениях величины $\theta$ с помощью прибора (метода), имеющего известную среднюю погрешность (стандартную ошибку) $\sigma$.

Эффективной оценкой (несмещенной с минимальной дисперсией) $\theta$ будет выборочное среднее:

$$\overline{\epsilon} \sim N(\theta, \frac{\sigma^2}{n}) \text{ - стандартизованная СВ}$$

$$\frac{\sqrt{n}(\overline{\epsilon} - \theta)}{\sigma} \sim N(0,1)$$

В качестве границ доверительного интервала с доверительной вероятностью $(1 - \alpha)$ можно взять:

$$\hat{\theta}_{min} = \overline{\epsilon} - \frac{\sigma}{\sqrt{n}}U_{1 - \alpha/2}$$

$$\hat{\theta}_{max} = \overline{\epsilon} - \frac{\sigma}{\sqrt{n}}U_{\alpha/2}$$

где $U_{\alpha}$ - квантиль нормального распределения уровня $\alpha$.

## 2. Метод моментов

Метод моментов используется для широкого класса моделей, при условии существования всех теоретических моментов с 1-го по m-й, где m - число оцениваемых параметров.

- $\mu_{k}$ - начальный момент k-го порядка.
- $m_{k}$ - выборочный начальный момент.

Если $\mu_{k}$ существует, то по закону больших чисел (ЗБУ):

$$m_{k} \to \mu_{k}, \text{ } n \to \infty$$

Пусть распределение выборки зависит от m параметров:

$$\theta = (\theta_{1}, \dots, \theta_{m}) \in \Theta$$

где $\Theta$ - некоторая область в $R^{m}$. 

Если существуют:

$$\mu_{k} = \mu(\theta), k = \overline{1,m}$$

можно записать систему нелинейных уравнений:

$$\mu_{k}(\theta) = y_{k}$$

Пусть левая часть системы задает взаимно-однозначное отображение:

$$g: \Theta \to B$$

где $B$ - некоторая область в $R^{m}$, и обратное отображение:

$$g^{-1}: B \to \Theta \text{ непрерывно}$$

Для всех $y$ из $B$ система имеет единственное решение, которое непрерывно зависит от правой части. 

Компоненты решения системы:

$$\hat{\theta} = (\hat{\theta_{1}}, \dots, \hat{\theta_{m}})$$

при $y_{k} = m_{k}$ называются оценками метода моментов (MM), где $\theta_{m}$ - случайные величины, так как $m_{k}$ - функция от случайной выборки.

### Примеры

**Пример 1.** Непрерывное равномерное распределение на $[0, \theta]$:

$$M\epsilon = \frac{\theta}{2} = \overline{x} \to \hat{\theta} = 2\overline{x}$$

**Пример 2.** Гамма распределение $\theta_{1}, \theta_{2}$:

$$\mu_{1} = \frac{\theta_{2}}{\theta_{1}} = \overline{x}$$

$$\theta_{2} = \frac{\theta_{2}(\theta_{2}+1)}{\theta^2} = \overline{x^2}$$

$$\hat{\theta_{2}} = \frac{(\overline{x})^2}{S^2}$$

$$\hat{\theta_{1}} = \frac{\overline{x}}{S^2}$$

## 3. Метод максимального правдоподобия (ММП)

Для простоты предположим дискретное распределение:

$$P(\epsilon_{i} = x) = f(x, \theta)$$

Совместная вероятность выборки:

$$f(x_{1}, \dots, x_{n}, \theta) = f(x_{1}, \theta) * \dots * f(x_{n}, \theta)$$

зависит от $n + m$ аргументов. Если её рассматривать как функцию от $\theta$ при фиксированной выборке, то она называется **функцией правдоподобия**:

$$L(\theta) = \prod f(x_{1}, \theta)$$

Разумно в качестве оценок взять такие значения, которые обеспечивают максимум правдоподобия:

$$argmax L(\theta) \text{ - оценка ММП}$$

Удобно работать с логарифмом функции правдоподобия: $\ln L(\theta)$.

### Примеры

**Пример 1:** Схема Бернулли с вероятностью успеха $\theta$:

$$\ln L(\theta) = \ln \theta \sum_{i=1}^{n}x_{i} + \left( n - \sum_{i=1}^{n}x_{i} \right)\ln(1 - \theta)$$

$$\frac{d\ln L(\theta)}{d\theta} = 0 \text{ - так можно делать, когда L - гладкая}$$

$$\hat{\theta} = \frac{\sum_{i=1}^{n}x_{i}}{n} = \overline{x}$$

**Пример 2:** Дискретное равномерное распределение на $[0, \theta]$:

$$P(\epsilon_{i} = x) = \frac{1}{\theta + 1}|_{0 \leq x \leq \theta}$$

$$L(\theta) = \frac{1}{(\theta + 1)^n}|_{X_{n} \leq \theta}$$

$$maxL(\theta) \text{ в точке } X_{(n)}$$

Оценка ММП в примере 2 смещена. Несмещенная оценка:

$$\hat{\theta} = \frac{n + 1}{n}x_{(n)}$$

Если $n \to \infty$, то оценка ММП - асимптотически несмещенная. Иногда решение можно найти явно, но чаще численно.

### Свойства ММП-оценок

1. Имеют наименьшую асимптотическую дисперсию (неравенство Рао-Крамера) $\to$ асимптотическая эффективность при выполнении условий регулярности.
2. При выполнении условий регулярности при больших и сильно состоятельных $\hat{\theta}_{n} \to \theta, n \to \theta$.

## Доверительные интервалы

Вместо точечной оценки (число) для $\theta$ можно указать интервал $(\hat{\theta}_{min}, \hat{\theta}_{max})$, в которых $\theta$ попадает с заданной (**доверительной**) вероятностью - доверительный интервал (или интервальная оценка).

Малая величина $\alpha = 1$ - доверительная вероятность, уровень значимости. Обычно $\alpha = 0.05; 0.01; 0.005$.

Статистики $\hat{\theta}_{min}, \hat{\theta}_{max}$ - границы интервала с доверительной вероятностью $(1 - \alpha)$, если $\forall \theta \in \Theta$ для выборки $\epsilon_1, \dots, \epsilon_{n}$ из $F_{\theta}(x)$ справедливо:

$$P(\hat{\theta}_{min}(\epsilon_{1}, \dots, \epsilon_{n}) < \theta < \hat{\theta}_{max}(\epsilon_{1}, \dots, \epsilon_{n})) \geq 1 - \alpha$$

При повышении доверительной вероятности длина интер# Методы математической статистики

## 2. Метод моментов

Метод моментов используется для широкого класса моделей, при условии существования всех теоретических моментов с 1-го по m-й, где m - число оцениваемых параметров.

- $\mu_{k}$ - начальный момент k-го порядка.
- $m_{k}$ - выборочный начальный момент.

Если $\mu_{k}$ существует, то по закону больших чисел (ЗБУ):

$$m_{k} \to \mu_{k}, \text{ } n \to \infty$$

Пусть распределение выборки зависит от m параметров:

$$\theta = (\theta_{1}, \dots, \theta_{m}) \in \Theta$$

где $\Theta$ - некоторая область в $R^{m}$. 

Если существуют:

$$\mu_{k} = \mu(\theta), k = \overline{1,m}$$

можно записать систему нелинейных уравнений:

$$\mu_{k}(\theta) = y_{k}$$

Пусть левая часть системы задает взаимно-однозначное отображение:

$$g: \Theta \to B$$

где $B$ - некоторая область в $R^{m}$, и обратное отображение:

$$g^{-1}: B \to \Theta \text{ непрерывно}$$

Для всех $y$ из $B$ система имеет единственное решение, которое непрерывно зависит от правой части. 

Компоненты решения системы:

$$\hat{\theta} = (\hat{\theta_{1}}, \dots, \hat{\theta_{m}})$$

при $y_{k} = m_{k}$ называются оценками метода моментов (MM), где $\theta_{m}$ - случайные величины, так как $m_{k}$ - функция от случайной выборки.

### Примеры

**Пример 1.** Непрерывное равномерное распределение на $[0, \theta]$:

$$M\epsilon = \frac{\theta}{2} = \overline{x} \to \hat{\theta} = 2\overline{x}$$

**Пример 2.** Гамма распределение $\theta_{1}, \theta_{2}$:

$$\mu_{1} = \frac{\theta_{2}}{\theta_{1}} = \overline{x}$$

$$\theta_{2} = \frac{\theta_{2}(\theta_{2}+1)}{\theta^2} = \overline{x^2}$$

$$\hat{\theta_{2}} = \frac{(\overline{x})^2}{S^2}$$

$$\hat{\theta_{1}} = \frac{\overline{x}}{S^2}$$

## 3. Метод максимального правдоподобия (ММП)

Для простоты предположим дискретное распределение:

$$P(\epsilon_{i} = x) = f(x, \theta)$$

Совместная вероятность выборки:

$$f(x_{1}, \dots, x_{n}, \theta) = f(x_{1}, \theta) * \dots * f(x_{n}, \theta)$$

зависит от $n + m$ аргументов. Если её рассматривать как функцию от $\theta$ при фиксированной выборке, то она называется **функцией правдоподобия**:

$$L(\theta) = \prod f(x_{1}, \theta)$$

Разумно в качестве оценок взять такие значения, которые обеспечивают максимум правдоподобия:

$$argmax L(\theta) \text{ - оценка ММП}$$

Удобно работать с логарифмом функции правдоподобия: $\ln L(\theta)$.

### Примеры

**Пример 1:** Схема Бернулли с вероятностью успеха $\theta$:

$$\ln L(\theta) = \ln \theta \sum_{i=1}^{n}x_{i} + \left( n - \sum_{i=1}^{n}x_{i} \right)\ln(1 - \theta)$$

$$\frac{d\ln L(\theta)}{d\theta} = 0 \text{ - так можно делать, когда L - гладкая}$$

$$\hat{\theta} = \frac{\sum_{i=1}^{n}x_{i}}{n} = \overline{x}$$

**Пример 2:** Дискретное равномерное распределение на $[0, \theta]$:

$$P(\epsilon_{i} = x) = \frac{1}{\theta + 1}|_{0 \leq x \leq \theta}$$

$$L(\theta) = \frac{1}{(\theta + 1)^n}|_{X_{n} \leq \theta}$$

$$maxL(\theta) \text{ в точке } X_{(n)}$$

Оценка ММП в примере 2 смещена. Несмещенная оценка:

$$\hat{\theta} = \frac{n + 1}{n}x_{(n)}$$

Если $n \to \infty$, то оценка ММП - асимптотически несмещенная. Иногда решение можно найти явно, но чаще численно.

### Свойства ММП-оценок

1. Имеют наименьшую асимптотическую дисперсию (неравенство Рао-Крамера) $\to$ асимптотическая эффективность при выполнении условий регулярности.
2. При выполнении условий регулярности при больших и сильно состоятельных $\hat{\theta}_{n} \to \theta, n \to \theta$.

## Доверительные интервалы

Вместо точечной оценки (число) для $\theta$ можно указать интервал $(\hat{\theta}_{min}, \hat{\theta}_{max})$, в которых $\theta$ попадает с заданной (**доверительной**) вероятностью - доверительный интервал (или интервальная оценка).

Малая величина $\alpha = 1$ - доверительная вероятность, уровень значимости. Обычно $\alpha = 0.05; 0.01; 0.005$.

Статистики $\hat{\theta}_{min}, \hat{\theta}_{max}$ - границы интервала с доверительной вероятностью $(1 - \alpha)$, если $\forall \theta \in \Theta$ для выборки $\epsilon_1, \dots, \epsilon_{n}$ из $F_{\theta}(x)$ справедливо:

$$P(\hat{\theta}_{min}(\epsilon_{1}, \dots, \epsilon_{n}) < \theta < \hat{\theta}_{max}(\epsilon_{1}, \dots, \epsilon_{n})) \geq 1 - \alpha$$

При повышении доверительной вероятности длина интервала повышается. Доверительный интервал зависит от распределения $\hat{\theta}$, которое зависит от распределения выборки. Самый простой случай - нормальная модель.

### Интервалы в нормальной модели

$$\epsilon_{i} \sim N(\theta, \sigma^2)$$

где $\sigma^2$ известно. Оценим параметр сдвига. Такая модель часто применяется к данным, полученным при независимых измерениях величины $\theta$ с помощью прибора (метода), имеющего известную среднюю погрешность (стандартную ошибку) $\sigma$.

Эффективной оценкой (несмещенной с минимальной дисперсией) $\theta$ будет выборочное среднее:

$$\overline{\epsilon} \sim N(\theta, \frac{\sigma^2}{n}) \text{ - стандартизованная СВ}$$

$$\frac{\sqrt{n}(\overline{\epsilon} - \theta)}{\sigma} \sim N(0,1)$$

В качестве границ доверительного интервала с доверительной вероятностью $(1 - \alpha)$ можно взять:

$$\hat{\theta}_{min} = \overline{\epsilon} - \frac{\sigma}{\sqrt{n}}U_{1 - \alpha/2}$$

$$\hat{\theta}_{max} = \overline{\epsilon} - \frac{\sigma}{\sqrt{n}}U_{\alpha/2}$$

где $U_{\alpha}$ - квантиль нормального распределения уровня $\alpha$.вала повышается. Доверительный интервал зависит от распределения $\hat{\theta}$, которое зависит от распределения выборки. Самый простой случай - нормальная модель.

### Интервалы в нормальной модели

$$\epsilon_{i} \sim N(\theta, \sigma^2)$$

где $\sigma^2$ известно. Оценим параметр сдвига. Такая модель часто применяется к данным, полученным при независимых измерениях величины $\theta$ с помощью прибора (метода), имеющего известную среднюю погрешность (стандартную ошибку) $\sigma$.

Эффективной оценкой (несмещенной с минимальной дисперсией) $\theta$ будет выборочное среднее:

$$\overline{\epsilon} \sim N(\theta, \frac{\sigma^2}{n}) \text{ - стандартизованная СВ}$$

$$\frac{\sqrt{n}(\overline{\epsilon} - \theta)}{\sigma} \sim N(0,1)$$

В качестве границ доверительного интервала с доверительной вероятностью $(1 - \alpha)$ можно взять:

$$\hat{\theta}_{min} = \overline{\epsilon} - \frac{\sigma}{\sqrt{n}}U_{1 - \alpha/2}$$

$$\hat{\theta}_{max} = \overline{\epsilon} - \frac{\sigma}{\sqrt{n}}U_{\alpha/2}$$

где $U_{\alpha}$ - квантиль нормального распределения уровня $\alpha$.